<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>OpenPose-Plus: poseplus::dnn::tensorrt Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">OpenPose-Plus
   </div>
   <div id="projectbrief">High-Performance and Flexible Pose Estimation Framework using TensorFlow, TensorRT and Stream Processing.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceposeplus.html">poseplus</a></li><li class="navelem"><a class="el" href="namespaceposeplus_1_1dnn.html">dnn</a></li><li class="navelem"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html">tensorrt</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classposeplus_1_1dnn_1_1tensorrt-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">poseplus::dnn::tensorrt Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p><code>tensorrt</code> is a class using TensorRT DNN engine to perform neural network inference.  
 <a href="classposeplus_1_1dnn_1_1tensorrt.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="tensorrt_8hpp_source.html">tensorrt.hpp</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a855ab99f6dbaa75a83892a952aeeac33"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#a855ab99f6dbaa75a83892a952aeeac33">tensorrt</a> (const std::string &amp;model_path, cv::Size <a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#a334cad82a2f362e56491c2a1139aff64">input_size</a>, const std::string &amp;input_name, const std::vector&lt; std::string &gt; &amp;output_names, int <a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#ab70a033708e8a72b14e9d34b81c80925">max_batch_size</a>=8, nvinfer1::DataType dtype=nvinfer1::DataType::kFLOAT, double factor=1./255, bool flip_rgb=true)</td></tr>
<tr class="memdesc:a855ab99f6dbaa75a83892a952aeeac33"><td class="mdescLeft">&#160;</td><td class="mdescRight">The constructor of TensorRT engine.  <a href="#a855ab99f6dbaa75a83892a952aeeac33">More...</a><br /></td></tr>
<tr class="separator:a855ab99f6dbaa75a83892a952aeeac33"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac6303bc19cafe046aa0cda439c1f5459"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#ac6303bc19cafe046aa0cda439c1f5459">~tensorrt</a> ()</td></tr>
<tr class="memdesc:ac6303bc19cafe046aa0cda439c1f5459"><td class="mdescLeft">&#160;</td><td class="mdescRight">Deconstructor of class <a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html" title="tensorrt is a class using TensorRT DNN engine to perform neural network inference. ">poseplus::dnn::tensorrt</a>.  <a href="#ac6303bc19cafe046aa0cda439c1f5459">More...</a><br /></td></tr>
<tr class="separator:ac6303bc19cafe046aa0cda439c1f5459"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab70a033708e8a72b14e9d34b81c80925"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#ab70a033708e8a72b14e9d34b81c80925">max_batch_size</a> () noexcept</td></tr>
<tr class="separator:ab70a033708e8a72b14e9d34b81c80925"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a334cad82a2f362e56491c2a1139aff64"><td class="memItemLeft" align="right" valign="top">cv::Size&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#a334cad82a2f362e56491c2a1139aff64">input_size</a> () noexcept</td></tr>
<tr class="separator:a334cad82a2f362e56491c2a1139aff64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aec0725ab4601bb7700796968e519c6d9"><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="namespaceposeplus.html#a256245f6f6861e752b11328ddd75c73b">internal_t</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#aec0725ab4601bb7700796968e519c6d9">inference</a> (std::vector&lt; cv::Mat &gt; inputs)</td></tr>
<tr class="memdesc:aec0725ab4601bb7700796968e519c6d9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Do inference with <code>cv::Mat</code>(OpenCV image/matrix data structure).  <a href="#aec0725ab4601bb7700796968e519c6d9">More...</a><br /></td></tr>
<tr class="separator:aec0725ab4601bb7700796968e519c6d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a56d7dbaf9f9c1c3060fdd61151e910a9"><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="namespaceposeplus.html#a256245f6f6861e752b11328ddd75c73b">internal_t</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#a56d7dbaf9f9c1c3060fdd61151e910a9">inference</a> (const std::vector&lt; float &gt; &amp;float_buffer, size_t batch_size)</td></tr>
<tr class="memdesc:a56d7dbaf9f9c1c3060fdd61151e910a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Do inference using plain float buffers(NCHW format required).  <a href="#a56d7dbaf9f9c1c3060fdd61151e910a9">More...</a><br /></td></tr>
<tr class="separator:a56d7dbaf9f9c1c3060fdd61151e910a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p><code>tensorrt</code> is a class using TensorRT DNN engine to perform neural network inference. </p>

<p class="definition">Definition at line <a class="el" href="tensorrt_8hpp_source.html#l00024">24</a> of file <a class="el" href="tensorrt_8hpp_source.html">tensorrt.hpp</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a855ab99f6dbaa75a83892a952aeeac33"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a855ab99f6dbaa75a83892a952aeeac33">&#9670;&nbsp;</a></span>tensorrt()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">poseplus::dnn::tensorrt::tensorrt </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>model_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">cv::Size&#160;</td>
          <td class="paramname"><em>input_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>input_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::string &gt; &amp;&#160;</td>
          <td class="paramname"><em>output_names</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>max_batch_size</em> = <code>8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">nvinfer1::DataType&#160;</td>
          <td class="paramname"><em>dtype</em> = <code>nvinfer1::DataType::kFLOAT</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>factor</em> = <code>1./255</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>flip_rgb</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The constructor of TensorRT engine. </p>
<dl class="section note"><dt>Note</dt><dd>Currently, we support the <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/uff/uff.html"><code>.uff</code></a> files which users should specify the input and output nodes by indicating their names(the names can be inferred when converting models to <code>.uff</code> format).</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model_path</td><td>Path to the model file. (currently, only <code>.uff</code> is supported) </td></tr>
    <tr><td class="paramname">input_size</td><td>The input size(width first, and then height) of the model. </td></tr>
    <tr><td class="paramname">input_name</td><td>The name of input node. (for simplicity, we only consider 1 input node model) </td></tr>
    <tr><td class="paramname">output_names</td><td>The names of output nodes. (e.g., openpose model will generate "paf" and "conf") </td></tr>
    <tr><td class="paramname">max_batch_size</td><td>The maximum batch size of the inputs. (for input/output buffer allocation) </td></tr>
    <tr><td class="paramname">dtype</td><td>The data type of data element. (for some GPUs, low precision data type will be faster) </td></tr>
    <tr><td class="paramname">factor</td><td>For each element in the input data, they will be multiplied by "factor". </td></tr>
    <tr><td class="paramname">flip_rgb</td><td>Whether to convert the color channels from "BGR" to "RGB". </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ac6303bc19cafe046aa0cda439c1f5459"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac6303bc19cafe046aa0cda439c1f5459">&#9670;&nbsp;</a></span>~tensorrt()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">poseplus::dnn::tensorrt::~tensorrt </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Deconstructor of class <a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html" title="tensorrt is a class using TensorRT DNN engine to perform neural network inference. ">poseplus::dnn::tensorrt</a>. </p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="aec0725ab4601bb7700796968e519c6d9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aec0725ab4601bb7700796968e519c6d9">&#9670;&nbsp;</a></span>inference() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="namespaceposeplus.html#a256245f6f6861e752b11328ddd75c73b">internal_t</a>&gt; poseplus::dnn::tensorrt::inference </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; cv::Mat &gt;&#160;</td>
          <td class="paramname"><em>inputs</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Do inference with <code>cv::Mat</code>(OpenCV image/matrix data structure). </p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>pp = <a class="code" href="namespaceposeplus.html">poseplus</a>;</div><div class="line"></div><div class="line"><span class="comment">// Create engine.</span></div><div class="line">pp::dnn::tensorrt engine(...);</div><div class="line"></div><div class="line"><span class="comment">// Prepare the input data.</span></div><div class="line"><span class="keyword">auto</span> mat = cv::imread(<span class="stringliteral">&quot;/path/to/images&quot;</span>);</div><div class="line"></div><div class="line"><span class="comment">// Inference 4 images.</span></div><div class="line">engine.inference({mat, mat, mat, mat});</div></div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">inputs</td><td>A vector of inputs. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section pre"><dt>Precondition</dt><dd><code>inputs.size() &lt;= <a class="el" href="classposeplus_1_1dnn_1_1tensorrt.html#ab70a033708e8a72b14e9d34b81c80925">max_batch_size()</a></code>(or <code>std::logic_error</code> will be thrown). </dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">std::logic_error</td><td></td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A vector of output feature maps(tensors). </dd></dl>

</div>
</div>
<a id="a56d7dbaf9f9c1c3060fdd61151e910a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a56d7dbaf9f9c1c3060fdd61151e910a9">&#9670;&nbsp;</a></span>inference() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="namespaceposeplus.html#a256245f6f6861e752b11328ddd75c73b">internal_t</a>&gt; poseplus::dnn::tensorrt::inference </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; float &gt; &amp;&#160;</td>
          <td class="paramname"><em>float_buffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batch_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Do inference using plain float buffers(NCHW format required). </p>
<p>This step will not involve in any scalar multiplication or channel swapping(related to the <code>factor</code> and <code>flip_rgb</code> parameter in the constructor).</p>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="namespaceposeplus.html#abfa82e50cd332c446bd3dfad91318519">poseplus::nhwc_images_append_nchw_batch</a> </dd>
<dd>
poseplus::tensorrt::tensorrt</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">float_buffer</td><td>The input float buffers. </td></tr>
    <tr><td class="paramname">batch_size</td><td>The batch size of inputs to do inference. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>vector of output feature maps(tensors). </dd></dl>

</div>
</div>
<a id="a334cad82a2f362e56491c2a1139aff64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a334cad82a2f362e56491c2a1139aff64">&#9670;&nbsp;</a></span>input_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">cv::Size poseplus::dnn::tensorrt::input_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="section return"><dt>Returns</dt><dd>The input <code>(height, width)</code> of this engine. </dd></dl>

<p class="definition">Definition at line <a class="el" href="tensorrt_8hpp_source.html#l00057">57</a> of file <a class="el" href="tensorrt_8hpp_source.html">tensorrt.hpp</a>.</p>

</div>
</div>
<a id="ab70a033708e8a72b14e9d34b81c80925"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab70a033708e8a72b14e9d34b81c80925">&#9670;&nbsp;</a></span>max_batch_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int poseplus::dnn::tensorrt::max_batch_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="section return"><dt>Returns</dt><dd>The maximum batch size of this engine. </dd></dl>

<p class="definition">Definition at line <a class="el" href="tensorrt_8hpp_source.html#l00053">53</a> of file <a class="el" href="tensorrt_8hpp_source.html">tensorrt.hpp</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>/home/ganler/Desktop/GANLER/codes/mine/openpose-plus/include/openpose_plus/operator/dnn/<a class="el" href="tensorrt_8hpp_source.html">tensorrt.hpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
